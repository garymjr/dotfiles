model = "gpt-5.3-codex"
tool_output_token_limit = 25000
# Leave room for native compaction near the 272–273k context window.
# Formula: 273000 - (tool_output_token_limit + 15000)
# With tool_output_token_limit=25000 ⇒ 273000 - (25000 + 15000) = 233000
model_auto_compact_token_limit = 233000
model_reasoning_effort = "high"
suppress_unstable_features_warning = true
approval_policy = "never"
sandbox_mode = "danger-full-access"
personality = "pragmatic"

[features]
ghost_commit = false
unified_exec = true
apply_patch_freeform = true
skills = true
shell_snapshot = true
steer = true
multi_agent = true
prevent_idle_sleep = true
voice_transcription = true

[agents]
max_threads = 12
max_depth = 2

[notice]
hide_full_access_warning = true

[tui]
theme = "catppuccin-mocha"
